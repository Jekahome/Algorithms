# Algorithms

* [Зачем изучать алгоритмы?](https://github.com/Jekahome/Algorithms#зачем-изучать-алгоритмы)
* [Книги про алгоритмы](https://github.com/Jekahome/Algorithms#книги-про-алгоритмы)
* [Алгоритмические метод решения задач](https://github.com/Jekahome/Algorithms#алгоритмические-метод-решения-задач)
* [Sorting Algorithms](https://github.com/Jekahome/Algorithms#sorting-algorithms)
* [Searching Algorithms](https://github.com/Jekahome/Algorithms#searching-algorithms)
* [Bit Manipulation/Bit Masking](https://github.com/Jekahome/Algorithms#bit-manipulation--bit-masking)
* [Комбинаторика](https://github.com/Jekahome/Algorithms#комбинаторика)
* [Задача NP-полная](https://github.com/Jekahome/Algorithms#задача-np-полная-np-complete-problem)
* [Асимптотическая сложность алгоритмов](https://github.com/Jekahome/Algorithms#асимптотическая-сложность-алгоритмов)
* [Асимптотическая сложность ф-ций в нотации Big-O](https://github.com/Jekahome/Algorithms#асимптотическая-сложность-ф-ций-в-нотации-big-o)
* [Top 20 LeetCode Patterns](https://github.com/Jekahome/Algorithms#top-20-leetcodepatterns)

## Зачем изучать алгоритмы?
- Алгоритмы - это про способ мыслить о программах
- Вы сможете писать оптимизированный и масштабируемый код - получив знания о различных структурах данных и алгоритмах, вы можете определить, какую выбрать структуру данных и алгоритм в различных условиях.
- Эффективное использование времени и памяти. Знание структур данных и алгоритмов поможет вам писать код, который работает быстрее и требует меньшего объема памяти.
 
## [Книги про алгоритмы](https://elbrusboot.camp/blog/top-11-knigh-po-alghoritmam/)
- Введение в алгоритмы, *Томас Х. Кормен* - это одна из лучших книг по алгоритмам, в которой подробно рассматривается широкий спектр алгоритмов.
- "Algorithms" *Роберта Седжвика* - это ведущий учебник по алгоритмам, широко используемый в колледжах и университетах.
- Искусство программирования, *Дональд Э. Кнут* - эта книга считается лучшей, если вы знакомы с предметом и ищете более глубокое понимание.
- "Introduction to Algorithms" (Томас Кормен, Чарльз Лейзерсон, Рональд Ривест, Клифорд Штайн) Эта книга, часто называемая CLRS по фамилиям авторов, является одной из самых известных и популярных книг по алгоритмам. Она охватывает широкий спектр тем и подходяща как для начинающих, так и для продвинутых студентов.

## Алгоритмические методы решения задач:

- **Brute-Force** - перебирайте все варианты в лоб.

    В чем разница между алгоритмом `Backtracking Algorithm` и алгоритмом `Brute-force`?

    В связи с тем, что алгоритм поиска `Backtracking` принимает для принятия решения все возможные результаты, с этой точки зрения он подобен алгоритму `Brute-force`. Разница состоит в том, что иногда `Backtracking` может обнаружить, что полный перебор не нужен, и, следовательно, может работать намного лучше.

<details>
<summary>...</summary>

- **Divide and Conquer** - разбивайте задачу на подзадачи и комбинируйте их (используют в алгоримах `Merge sort` и `Binary Search`). Разделяй и властвуй — это парадигма разработки алгоритма, основанная на многоветвящейся рекурсии. Алгоритм `Divide and Conquer` разбивает проблему на подзадачи того же или родственного типа, пока они не станут достаточно простыми, чтобы их можно было решить напрямую.

    Алгоритмы «разделяй и властвуй» представляют собой парадигму решения задач, включающих несколько основных шагов. Сначала мы делим проблему на более мелкие части и работаем над решением каждой из них независимо. После того, как мы решили все части, мы берем все полученные более мелкие решения и объединяем их в одно интегрированное комплексное решение.

    Этот процесс может выполняться рекурсивно; то есть каждая «подзадача» сама по себе при необходимости может быть разделена на еще более мелкие части. Это рекурсивное деление задачи выполняется до тех пор, пока каждая отдельная проблема не станет достаточно маленькой, чтобы ее решение стало относительно тривиальным.

    Некоторыми распространенными примерами задач, которые хорошо подходят для этого подхода, являются `Binary Search`, алгоритмы сортировки (например, `Merge sort`, `Quicksort`), оптимизация сложных в вычислительном отношении математических операций (возведение в степень, БПФ, алгоритм Штрассена) и другие.

    Ниже приведены некоторые стандартные алгоритмы, соответствующие алгоритму `Divide and Conquer`.  

    - `Quicksort` — это алгоритм сортировки. Алгоритм выбирает опорный элемент и переупорядочивает элементы массива так, чтобы все элементы, меньшие, чем выбранный опорный элемент, перемещались в левую сторону опорного элемента, а все элементы большего размера перемещались в правую сторону. Наконец, алгоритм рекурсивно сортирует подмассивы слева и справа от опорного элемента.
    - `Merge sort` также является алгоритмом сортировки. Алгоритм делит массив на две половины, рекурсивно сортирует их и, наконец, объединяет две отсортированные половины.
    - Ближайшая пара точек. Задача состоит в том, чтобы найти ближайшую пару точек в наборе точек в плоскости `xy`. Проблему можно решить за время `O(n^2)`, вычислив расстояния до каждой пары точек и сравнив расстояния, чтобы найти минимум. Алгоритм «разделяй и властвуй» решает проблему за время `O(N log N)`.
    - Алгоритм Штрассена — это эффективный алгоритм умножения двух матриц. Простой метод умножения двух матриц требует трех вложенных циклов и равен `O(n^3)`. Алгоритм Штрассена умножает две матрицы за время `O(n^2,8974)`.
    - Алгоритм быстрого преобразования Фурье (БПФ) Кули – Тьюки является наиболее распространенным алгоритмом БПФ. Это алгоритм «разделяй и властвуй», который работает за время `O(N log N)`.
    - Алгоритм Карацубы для быстрого умножения выполняет умножение двух n - значных чисел  

    ```
    DAC(a, i, j) {
        if(small(a, i, j))
            return(Solution(a, i, j))
        else 
            mid = divide(a, i, j)   // f1(n)
            b = DAC(a, i, mid)      // T(n/2)
            c = DAC(a, mid+1, j)    // T(n/2)
            d = combine(b, c)       // f2(n)
        return(d)
    }
    ```

    [geeksforgeeks](https://www.geeksforgeeks.org/complete-roadmap-to-learn-dsa-from-scratch/)

- **Dynamic Programming** - Кешируйте промежуточные результаты для их повторного использования. `DP = Recursion + some memory`

    Подход `Memoization` (`Сверху Вниз`) - кеширования результатов повторяющихся расчетов в таблицу и их использования по мере хода алгоритма.
    
    Подход `Tabulation` (`Снизу Вверх`) - кеширование всех результатов подзадач в таблицу и после в нужной последовательности сбор результатов. Пример, задача о максимальной вместимости рюкзака, для перебора всех возможных вариантов `O(2^N)`, быстрее решать подзадачи `O(N^2)`.

    Подход `Memoization`. Функции могут использовать объекты для запоминания результатов предыдущих операций, что позволяет избежать ненужной работы. Такая оптимизация называется мемоизацией .
    Мемоизация Фибоначчи. Алгоритм вычисления чисел Фибоначчи. 
    Его дерево рекурсивных вызовов показывает, что fib(3) вычисляется многократно. 
    Мы можем это исправить, сохраняя результаты по мере их вычисления и делая новые вызовы fib только для тех вычислений, результатов которых еще нет в памяти многократного использования промежуточных результатов называется мемоизацией.

    [Как классифицировать проблему как проблему динамического программирования?](https://www.geeksforgeeks.org/solve-dynamic-programming-problem/?ref=lbp)

    Как правило, все проблемы, требующие максимизации или минимизации определенных величин, или задачи подсчета, требующие подсчета композиций при определенных условиях, или определенные проблемы вероятности, могут быть решены с помощью динамического программирования.
    
    Все задачи динамического программирования удовлетворяют свойству перекрывающихся подзадач, а большинство классических задач динамического программирования также удовлетворяют свойству оптимальной подструктуры. Как только мы обнаружим эти свойства в данной задаче, будьте уверены, что ее можно решить с помощью динамического программирования.

    [Dynamic Programming topcoder](https://www.topcoder.com/thrive/articles/Dynamic%20Programming:%20From%20Novice%20to%20Advanced) - Кешируйте промежуточные результаты для их повторного использования. 

    [Dynamic Programming brestprog](https://brestprog.by/topics/dp/)

    [Dynamic Programming brestprog](https://brestprog.by/topics/bitmasks/)

    [Dynamic Programming topcoder](https://www.topcoder.com/thrive/tracks?tax=Dynamic%20Programming&track=Competitive%20Programming)

    [Динамическое программирование](https://ru.algorithmica.org/cs/general-dynamic/)

- **Greedy Algorithm** - пытайтесь взять что-то выгодное в первую очередь (жадный алгоритм). Жадные алгоритмы используются для решения задач оптимизации, делая локально оптимальный выбор на каждом шаге. В этих алгоритмах решения принимаются на основе информации, доступной в текущий момент, без учета последствий этих решений в будущем. Основная идея состоит в том, чтобы на каждом этапе выбрать наилучший возможный вариант, ведущий к решению, которое не всегда может быть самым оптимальным, но часто достаточно хорошим для многих проблем.

    Например: 
    - Кодирование Хаффмана — это алгоритм сжатия данных, который формулирует основную идею сжатия файлов.
    - Проблема размена монет. Жадный алгоритм можно использовать для сдачи заданной суммы с минимальным количеством монет, всегда выбирая монету с наибольшей стоимостью, которая меньше оставшейся суммы, подлежащей обмену.


    [Greedy Algorithm](https://www.geeksforgeeks.org/greedy-algorithms/)

    [Introduction to Greedy Algorithm](https://www.geeksforgeeks.org/introduction-to-greedy-algorithm-data-structures-and-algorithm-tutorials/)

    [Greedy Algorithm](https://www.programiz.com/dsa/greedy-algorithm)

- **Blacktracking** - (поиск с возвратом) оптимизация `Brute-Force` отменяя отдельные затратные операции еще до начала их выполнения. Т.е. отбросить заведомо неверный путь, недопустив затрат на его расчет еще до начала самого расчета. Это общий метод нахождения решений задачи, в которой требуется полный перебор всех возможных вариантов в некотором множестве М. 
Это сама по себе рекурсия, но здесь есть некоторые дополнительные условия, которые делают ее более эффективной.

    Проблемы, которые обычно решаются с использованием метода обратного отслеживания, имеют следующее общее свойство. Эти проблемы можно решить, только перепробовав все возможные конфигурации, причем каждая конфигурация проверяется только один раз. Наивное решение этих проблем — попробовать все конфигурации и вывести конфигурацию, соответствующую заданным ограничениям задачи. Обратное отслеживание работает постепенно и представляет собой оптимизацию по сравнению с решением `Naive`, при котором генерируются и опробуются все возможные конфигурации.

    Как упоминалось ранее, алгоритм `Blacktracking` является производным от алгоритма рекурсии и имеет возможность вернуться в исходное состояние в случае сбоя рекурсивного решения, т. е. в случае сбоя решения программа возвращается к моменту, когда оно потерпело неудачу, и основывается на другом решении. По сути, он пробует все возможные решения и находит правильное.

    Обход с возвратом — это алгоритмический метод решения проблем рекурсивно, пытаясь построить решение постепенно, по одному фрагменту за раз, удаляя те решения, которые не удовлетворяют ограничениям проблемы в любой момент времени (под временем здесь подразумевается время, прошедшее до достижения любого уровня дерева поиска).

    Некоторые стандартные проблемы:
    - Задача решения судоку
    - Крыса в лабиринте
    - Сумма комбинации, Сумма комбинации II, Сумма комбинации III
    - Проблема с королевой N
    - Проблема N-ферзей

- **Local Search** - пытайтесь выбрать решение которое не хуже предыдущего (градиентный спуск/подьем).

    Примером [локального поиска](https://www.simplilearn.com/local-search-algorithms-in-ai-article#:~:text=Local%20search%20algorithms%20focus%20on%20finding%20solutions%20within%20a%20limited,explore%20the%20entire%20solution%20space.) является алгоритм «Восхождение на холм». Он начинается с первоначального решения и итеративно вносит небольшие изменения для улучшения текущего решения с целью найти локально оптимальное решение в ограниченной части пространства решений.

- **Transform and Conquer** - преобразование данных для лучшего их использования или преобразование самой задачи (индекс в базах данных, разместить данные особым способом для быстрого поиска. Инвертированные индекс и Column-oriented DB MS).

    [Способы применения:](https://www.geeksforgeeks.org/transform-and-conquer-technique/)
   
    1. Упрощение экземпляра: Это можно сделать, уменьшив размер проблемы, разбив ее на более мелкие части или изменив структуру проблемы. Этот метод можно использовать для упрощения задач, которые слишком велики для решения традиционными методами.
   
    2. Уменьшение проблемы: Идея сокращения проблем состоит в том, чтобы превратить данную проблему в другую, которую легче решить. Это можно сделать, преобразовав проблему в другую форму или используя эвристику для поиска решения.

    3. Изменение представления: Основная идея этого метода — изменить представление данных, чтобы их можно было упростить. Это можно сделать путем изменения входных данных или выходных данных.

 
- **Randomized algorithm** - (рандомизированный алгоритм) используют случайность для решения проблемы. Это может быть полезно для решения проблем, которые не могут быть решены детерминистически, или для повышения средней сложности задачи. Рандомизированная быстрая сортировка: вариант алгоритма быстрой сортировки, в котором точка опоры выбирается случайным образом.

    [Randomized algorithm](https://www.geeksforgeeks.org/randomized-algorithms/)

</details>

## Sorting Algorithms

Сортировка является фундаментальной операцией в компьютерных науках, и для неё существует несколько эффективных алгоритмов, таких как `Quicksort`, `Merge sort` и `Heapsort`.

[Radix Sort](https://thecode.media/radix/) — сортировка по основанию системы счисления.

Главное коротко: алгоритм поразрядной сортировки гениален в том, что сортирует не числа целиком, а значения разрядов. Получается, что он как бы разбирается с числами на уровне единиц, десятков, сотен и т. д. и только потом он делает общую сортировку. Это позволяет ему не бегать по всем сравниваемым числам и не делать миллион сравнений. Отсюда и экономия времени.

[Временная сложность Radix Sort](https://www.geeksforgeeks.org/radix-sort/) `O(d * (n + b))` , где d — количество цифр, n — количество элементов, а b — основа используемой системы счисления.

В практических реализациях поразрядная сортировка `Radix Sort` часто работает быстрее, чем другие алгоритмы сортировки на основе сравнения, такие как `Quicksort` или `Merge sort`, для больших наборов данных, особенно когда ключи содержат много цифр. Однако его временная сложность растет линейно с увеличением количества цифр, поэтому он не так эффективен для небольших наборов данных.

Вспомогательное пространство `Radix Sort` `O(n + b)`, где n — количество элементов, а b — основание системы счисления. 


[Sorting Algorithms](https://www.geeksforgeeks.org/sorting-algorithms/)

[Counting sort](https://www.geeksforgeeks.org/counting-sort/)

![Sort algorithms.](https://github.com/Jekahome/Algorithms/blob/main/_img/algo_sort.jpg "Sort algorithms.")

 
## Searching Algorithms

Поиск элемента в большом наборе данных — распространенная задача, и для неё существует несколько эффективных алгоритмов, таких как бинарный поиск и хеш-таблицы.

<details>
<summary>...</summary>

Наиболее распространенные алгоритмы поиска:

- [`Linear Search`](https://www.geeksforgeeks.org/linear-search/) (Линейный поиск) `O(n)` - проверяем элемент итеративно от одного конца к другому.
- [`Binary Search`](https://www.geeksforgeeks.org/binary-search/) (Двоичный поиск) `O(log n)` - разбиваем структуру данных на две равные части и пытаемся решить, в какой половине нам нужно найти элемент. 
- [`Ternary Search`](https://www.geeksforgeeks.org/ternary-search/) (Тернарный поиск) `O(2 * log3n)` - массив делится на три части, и на основе значений в позициях разделения мы определяем сегмент, в котором нам нужно найти нужный элемент.
- [`Jump Search`](https://www.geeksforgeeks.org/jump-search/) между `O(n)` и двоичным поиском `O(Log n)` - это алгоритм поиска в отсортированных массивах. Основная идея состоит в том, чтобы проверять меньше элементов (по сравнению с линейным поиском ), переходя вперед на фиксированные шаги или пропуская некоторые элементы вместо поиска по всем элементам. Если мы сравним его с линейным и бинарным поиском, то окажется, что он лучше, чем линейный поиск, но не лучше, чем бинарный поиск.
- [`Interpolation Search`](https://www.geeksforgeeks.org/interpolation-search/) `O(log 2 (log 2 n))` для среднего случая и `O(n)` для наихудшего случая - это улучшение по сравнению с двоичным поиском для случаев, когда значения в отсортированном массиве распределены равномерно. Интерполяция создает новые точки данных в диапазоне дискретного набора известных точек данных. Двоичный поиск всегда обращается к среднему элементу для проверки. С другой стороны, интерполяционный поиск может осуществляться в разных местах в зависимости от значения искомого ключа. Например, если значение ключа ближе к последнему элементу, интерполяционный поиск, скорее всего, начнет поиск в направлении конечной стороны.
- [`Exponential Search`](https://www.geeksforgeeks.org/exponential-search/) `O(log n)` - Он работает лучше, чем двоичный поиск, для ограниченных массивов, а также когда искомый элемент находится ближе к первому элементу.

`Binary Search` и `Exponential Search` это алгоритмы поиска значения в отсортированном (подготовленном) наборе данных, но если характер использования подразумевает постоянное изменение набора данных, то эти алгоритмы будут иметь значительно меньшую производительность чем [`Red Black Tree`](https://github.com/Jekahome/Data-Structures/tree/main/src/red_black_tree)

![Searching Algorithms.](https://github.com/Jekahome/Algorithms/blob/main/_img/Searching-Algorithms.png "Searching Algorithms.")

[geeksforgeeks](https://www.geeksforgeeks.org/complete-roadmap-to-learn-dsa-from-scratch/)

</details>

### Фильтр Блума и HyperLogLog

Вероятностные структыры данных. Они дают ответ,который может оказаться ложным, но с большой вероятностью является правильным. 
Фильтры Блума очень удобны тогда, когда не нужно хранить точный ответ. Пример, что бы уменьшить количество проверок наличия ключа в базе можно получить вероятный ответ наличия или точно отрицательный используя Фильтр Блума

### Линейное программирование

Линейное программирование используется для максимизации некоторой характеристики при заданных ограничениях. 
Предположим, ваша компания выпускает два продукта: рубашки и сумки. На рубашку требуется 1 м 5 пуговиц. 
На изготовление сумки необходимо 2 м ткани и 2 пуговицы. У вас есть 11 м ткани и 20 пуговиц. 
Рубашка приносит прибыль $2, а сумка - $3. Сколько рубашек и сумок следует изготовить для получения ткани и максимальной прибыли?

## Bit Manipulation / Bit Masking:

Чтобы две цифры от 0-9 можно было закодировать в одном байте

```
AND (&)
OR (|)
XOR (^)
NOT (~)
Left Shift (<<)
Right Shift(>>)
```

[Data Compression: Bit-Packing 101](https://kinematicsoup.com/news/2016/9/6/data-compression-bit-packing-101)


## [Комбинаторика](https://practicum.yandex.ru/blog/perestanovki-razmescheniya-sochetaniya-v-analize-dannyh/)

Комбинато́рика — раздел математики, посвящённый решению задач, связанных с выбором и расположением элементов некоторого (чаще всего конечного) множества в соответствии с заданными правилами. 

## Задача NP-полная (NP-complete problem)

Тип задач, принадлежащих классу `NP` (non-deterministic polynomial – «недетерминированные с полиномиальным временем» `O(N^2)`, `O(N^3)`), для которых отсутствуют быстрые алгоритмы решения. Время работы алгоритмов решения таких задач существенно (обычно, экспоненциально) возрастает с увеличением объема входных данных.

Буква `P` в названии означает полиномиальную сложность алгоритма  `A_0X^n + ... + A_n`
Буква `N` - оценка сложности соответствует выполнению на недетерминированной машине Тьюринга, иначе — обычной.

Однако, если предоставить алгоритму некоторые дополнительные сведения, то временные затраты могут быть существенно снижены. При этом, если будет найден быстрый алгоритм для какой-либо из NP-полных задач, то для любой другой задачи из класса NP можно будет найти соответствующее решение.

К классу NP-полных относятся задача о коммивояжере, о вершинном покрытии и покрытии множеств, восстановление поврежденных файлов, оптимизация маршрутов, сложные вычисления в биоинформатике.

Криптография открытых ключей основывается на предположении, что `NP ≠ P`. Если найдется способ решать задачи этого класса за полиномиальное время, то многие методы защиты больше не будут иметь смысла.


## [Асимптотическая сложность алгоритмов](https://www.youtube.com/watch?v=GGXWAmCBetA)

Двумя фундаментальными понятиями в этой области являются временная сложность и пространственная сложность

`Big O` - описывает **наихудший** сценарий того, сколько времени потребуется алгоритму для решения проблемы по мере роста входных данных. Он отвечает на вопрос: "Что произойдет, если мы подадим на вход алгоритму огромный, гипотетически бесконечный объем данных?". Это и есть асимптотический анализ — анализ поведения на бесконечности. 

`Big O Space` - сколько памяти (пространства для хранения) требуется алгоритму для работы с входными данными.

Буква `О` представляет собой сокращение от слова order ("порядок"). Основным параметром `O-оценки` является `n`, т.е. размер исследуемой задачи, причем время выполнения алгоритма или его сложность представляется в виде функции `n`. 
 
Сложность алгоритма - это то, как будет **расти** потребление ресурсов с увеличением `N` количества елементов до бесконечности. Наивная реализация алгоритма/задачи которая превышает заданный лимит времени выполнения требует другого решения с учетом оптимизации количества операций и выделенных ресурсов. Если ваше решение не укладывается в ограничение по памяти, оно скорее всего не уложится и в ограничение по времени.

На практике возникают значительные отличия в деталях асимптотических оценок. Например, алгоритм порядка `O(n2)` с небольшим коэффициентом пропорциональности может работать быстрее, чем алгоритм порядка `O(n log n)` с высоким коэффициентом **для относительно малых `n`**, но по мере роста `n` преимущество будет оставаться за алгоритмом с медленнее растущей функцией в `O-оценке`. Следует также различать поведение алгоритмов в наихудшем и среднестатистическом случаях. Например, для алгоритма быстрой сортировки наихудший случай соответствует количеству операций порядка `O(n2)`, а его среднее быстродействие характеризуется оценкой `O(n log n)`. Всякий раз тщательно выбирая опорный элемент, можно свести вероятность квадратичного, т.е. `O(n2)`, объема операций практически к нулю. 


Количество операций для n = 10_000 елементов:

| **Оценка**   | **Характер зависимости**                                                                      | **Пример операции**                          |
| ------------ | --------------------------------------------------------------------------------------------- | -------------------------------------------- |
| `O(1)`       | Константная — 1 операция                                                                      | Обращение по индексу                         |
| `O(log n)`   | Логарифмическая (как в бинарном поиске) — ~13 операций                                        | Бинарный поиск                               |
| `O(√n)`      | Корневая √10 000 = 100 операций                                                               | Проверка простоты перебором делителей до √n  |
| `O(n)`       | Линейная — 10 000 операций (≈ 10 мкс при 1 нс/операцию)                                       | Полный перебор массива. Сравнение строк      |
| `O(n log n)` | Пропорциональная/Линейно-логарифмическая (quicksort) — 10 000 × 13 ≈ 130 000 (≈ 130 мкс при 1 нс/операцию)     | Быстрая сортировка          |
| `O(n²)`      | Квадратичная — 100 000 000 операций (≈ 0.1 с при 1 нс/операцию)                               | Сортировка вставками                         |
| `O(n³)`      | Кубическая (тройной вложенный цикл) — 1 000 000 000 000 операций (≈ 17 мин при 1 нс/операцию) | Умножение матриц                             |
| `O(2^n)`     | Экспоненциальная — 2^10 000 ≈ 10^3010 (недостижимо)                                           | Решение задачи коммивояжёра полным перебором |
| `O(n!)`      | Факториальная — 10 000 000 000 000 000 000 000 000 ... операций                               | Перебор всех перестановок                    |



![Comparison of algorithms.](https://github.com/Jekahome/Algorithms/blob/main/_img/algo.jpg "Comparison of algorithms.")

## Асимптотическая сложность ф-ций в нотации Big-O:
[Cronis Academy. Оценка сложности алгоритма. Сложность алгоритмов. Big O, Большое О](https://www.youtube.com/watch?v=ZRdOb4yR0kk&t=901s)

1. константная `O(1)` 
2. логарифмическая `O(log N)`, `O(log^2 N)`
3. корень из N `O(sqrt N)`
4. линейная `O(N)` 
5. линейная `O(N+M)` 
6. линеарифмическая/linearithmic `O(N*log N)`, `O(N*log^2 N)` или `O(N*M)`,`O(N*sqrt M)`
7. полиномиальная квадратичная `O(N^2)`,`O(N^2*log N)` или кубическая `O(N^3)` 
8. экспоненциальная `O(2^N)`
9. факториал `O(n!)`
 
<details>
<summary>Пренебрежение константами:</summary>

Почему мы говорим, что `O(2n + 5)` — это то же самое, что `O(n)`? Потому что при стремлении `n` к бесконечности константы 2 и 5 становятся пренебрежимо малыми.

Сравните: `n = 1 000 000. 2n + 5 = 2 000 005`. Это всего лишь примерно в 2 раза больше, чем n. Рост остается линейным.

А вот разница между `O(n)` и `O(n²)` при `n = 1 000 000` — это уже разница между `1 000 000` и `1 000 000 000 000`. Эта разница будет только бесконечно увеличиваться с ростом `n`

Так же:
| пример после расчета | реальная сложность | причина|
|----------------------|--------------------|------|
| `O(n² + n)`| `O(n²)`| `n²` значительно больше `n`|
| `O(n + log n)`|  `O(n)` | `n` значительно больше `log n`|
| `O(5 * 2^n + 10 * n^100)`| `O(2^n)`|`2^n` значительно больше `n^100`|
| `O(n² + B)`| `O(n² + B)`| если мы не знаем что такое `B`|

</details>


<details>
<summary>Как считать сложности:</summary>

**Сложность `O(A+B)`** - последовательный перебор независимых наборов, сперва набора данных A потом B 

```c
for (int a: arrA){
    print(a);
}
for (int b: arrB){
    print(b);
}
```

**Сложность `O(A*B)`** - зависимый перебор данных, на каждый елемент набоа A выполняется полный перебор набора B

```c
for (int a: arrA){
    for (int b: arrB){
        print(a, b);
    }
}

```

**Сложность `O(N+N)=O(N)`** - последовательный перебор зависимых наборов

```c
for (int a: arrA){
    print(a);
}
for (int a: arrA){
    print(a);
}
```



**Сложность `O(n²)`** - зависимый перебор данных, когда на каждый елемент набоа A выполняется полный перебор того же набора A

```c
for (int a: arrA){
    for (int b: arrA){
        print(a, b);
    }
}

```

**Сложность `O(√n)`** - когда перебор идет до корня из `n`. Вместо перебора всех `n` элементов, мы перебираем только `√n` элементов.

Что такое корень:

```
√n = число, которое умножили САМО НА СЕБЯ и получили n

√25 = 5, потому что 5 × 5 = 25
```

`O(√n)` гораздо лучше чем `O(n)`, но хуже чем `O(log n)`:

```c
// Проверяем, является ли число n простым
bool isPrime(int n) {
    if (n < 2) return false;
    
    // Перебираем делители только до sqrt(n)
    for (int i = 2; i * i <= n; i++) {
        if (n % i == 0) {
            return false;  // Нашли делитель - число не простое
        }
    }
    return true;  // Делителей не найдено - число простое
}
```


**Сложность `O(log n)`** - алгоритм, который отбрасывает половину данных на каждом шаге. На каждой итерации цикла или каждом шаге рекурсии алгоритм уменьшает объем задачи в 2 или более раз. Это очень эффективно, потому что количество шагов растет очень медленно даже при огромном `n`.

```c
int n = 100;
while (n > 1) {
    n = n / 2;  // На каждом шаге делим пополам
    printf("%d\n", n);
}
```

**Сложность `O(2^n)`** - когда на каждом элементе ветвимся на 2 варианта, это очень медленно. Уже при `n=60`, `2ⁿ` больше чем число атомов в наблюдаемой вселенной!

Самый простой пример: Все возможные подмножества множества:

```
// n = 3 элемента: {A, B, C}
// Все возможные подмножества:
{}
{A}
{B}
{C}
{A, B}
{A, C}
{B, C}
{A, B, C}
```


```c
void generateSubsets(int[] arr, int n) {
    // 2^n возможных подмножеств
    for (int i = 0; i < (1 << n); i++) {
        printf("{ ");
        for (int j = 0; j < n; j++) {
            // Проверяем j-й бит
            if (i & (1 << j)) {
                printf("%d ", arr[j]);
            }
        }
        printf("}\n");
    }
}
```

**Сложность `O(n!)`** - когда перебираем все возможные перестановки, это катастрофически медленно. Уже при `n=20` перебирать все варианты невозможно — их больше чем атомов во Вселенной!

`n = 10 → 10! = 3_628_800 перестановок`

Где встречается `O(n!)`:
* Задача коммивояжера (маршрут через N городов)
* Расстановка N ферзей на шахматной доске
* Все возможные анаграммы слова
* Любая задача "перебрать все варианты порядка"


</details>



<details>
<summary>Примеры:</summary>

```rust
/* 
Временная сложность алгоритма - константная O(1)
Это лучшее. 
Алгоритм всегда занимает одинаковое количество времени, независимо от объема данных. 
Пример: поиск элемента массива по его индексу.
*/
fn algo_1(v: &[i32], index: usize) -> Option<i32>{
    if index < v.len(){
        return Some(v[index]);
    }
    None    
} 
// или
let x = &[1, 2, 4];
unsafe {
    assert_eq!(x.get_unchecked(1), &2);
}

// или
vec.push(), vec.pop()

``` 

```rust
/* 
Временная сложность алгоритма - логарифмическая O(log N)
(`log N` это `log _2 N` в какой степени должна быть двойка чтобы получилось N, 
это и будет количество операций)
каждая итерация сокращает вдвое количество элементов/значений
Перевод числа в двоичное представление
TODO: Логарифм по основанию `a` от аргумента `x` — это степень, 
в которую надо возвести число `a`, чтобы получить число `x`

log_2 64 = 6 так как 2^6=64

Довольно здорово. Подобные алгоритмы уменьшают вдвое объем данных на каждой итерации. 
Если у вас 100 элементов, то чтобы найти ответ, потребуется около 7 шагов. 
При наличии 1000 элементов требуется 10 шагов. 
А для 1 000 000 элементов требуется всего 20 шагов. 
Это очень быстро даже для больших объемов данных. 
Пример: бинарный поиск.
*/
fn algo_2(mut decimal:u8) -> Option<String>{
    if decimal == 0 {return None;}
    let mut binary = String::from(""); 
    while decimal > 0 {
        binary = format!("{}{}",decimal%2,binary);
        decimal = decimal.div_floor(2);
    }
    Some(binary)
}

// или
let j = 1
while j < n {
  // do constant time stuff
  j *= 2
}

```

```rust
/* 
Временная сложность алгоритма - корень из N (sqrt N)
условие выхода из цикла x^2 следовательно цикл прекратится когда x >= sqrt N

TODO: Корень `n-й` степени из числа `a` определяется как такое число `b`, что `b^n=a` 
Здесь `n` — натуральное число, называемое показателем корня (или степенью корня); 
как правило, оно больше или равно 2, потому что случай `n=1` не представляет интереса.

Пример: Корнями 2-й степени из числа 9 являются +/-3  т.е. `9 sqrt^2=3`  
так как 3^2=9,а `64 sqrt^3=4` так как 4^3=64
И график O(sqrt N) будет расти быстрее, следовательно медленнее работать чем график O(log N)
так как для N=64 => `log_2 64 = 2^6 = 64 => 6` это < `64 sqrt^2 = 8^2 = 64 => 8`
т.е. для log мы двойку возводим в нужную степень, 
а для sqrt нужно само число для возведения в квадрат 
*/
fn algo_sqrt(v:&Vec<i32>){
    let mut x = 0;
    while x*x < v.len() {
        x+=1;
    }
}
```

```rust
/* 
Временная сложность алгоритма - линейная O(N)
Время выполнения увеличивается пропорционально размеру задачи.

Хорошая производительность. Если у вас 100 элементов, это выполняет 100 единиц работы. 
При удвоении количества элементов алгоритм будет выполняться ровно в два раза дольше (200 ед. работы). 
Пример: последовательный поиск.
*/
fn algo_3(v:&Vec<i32>,n:i32) -> bool{
    for i in v{
        if i == &n{
            return true;
        }
    }
    false
}
```

```rust
// Временная сложность алгоритма - линейная O(N + M) 
fn algo_5(n:&Vec<i32>,m:&Vec<i32>,element:&i32) -> Option<i32>{
    let mut value:i32 = 0;
    for i_n in n{
       if i_n > element{
         value = *i_n;
       }
    }
    if value == 0{return None};
    for i_m in m {
        if &value == i_m{
            return Some(*i_m);
        }
    }
    None
}
```

```rust
/*
Временная сложность алгоритма - линеарифмическая O(N * M) 

Достойная производительность. Это немного хуже линейного, но не так уж плохо. 
Пример: самые быстрые алгоритмы сортировки общего назначения.
*/
fn algo_6(n:&Vec<i32>,m:&Vec<i32>) -> Option<i32>{
    for i_n in n{
        for i_m in m{
            if i_n == i_m{
                return Some(*i_m);
            }
        }
    }
    None
}
```

```rust
/* 
Временная сложность алгоритма - полиномиальная (квадратичная) сложность O(N^2)
 1 + 2 + 3 + 4 + ... + N => O(N^2)
 N-1 * N-1 = N^2
Часто встречается в алгоритмах с вложенными циклами. 

Как-то медленно. Если у вас 100 элементов, это 100^2 = 10 000 единиц работы. 
Удвоение количества предметов делает процесс медленнее в четыре раза 
(поскольку 2 в квадрате равно 4). 
Пример: алгоритмы, использующие вложенные циклы, такие как сортировка вставками.
*/
fn algo_6_bubble_sort(v:&mut Vec<i32>){
    for i in 0..v.len()-1 {
        for j in 0..v.len()-1 {
            if v[j] > v[j+1]{
                let swap = v[j];
                v[j]=v[j+1];
                v[j+1]=swap;
            }
        }
    }
}

// или
for i in 0..n {
  for j in 0..n {
    ...
  }
}
```

```rust
/* 
Временная сложность алгоритма - полиномиальная (кубическая) сложность O(N^3)

Шаг за пределы квадратичной сложности. 
По мере увеличения размера задачи время выполнения увеличивается еще быстрее. 
Это происходит в алгоритмах с тремя вложенными циклами, например в алгоритмах 
с трехмерными массивами.

Низкая производительность. Если у вас 100 элементов, это 100^3 = 1 000 000 единиц работы.
Удвоение входного размера делает его в восемь раз медленнее. Пример: умножение матрицы.
*/
for i in 0..n {
  for j in 0..n {
    for k in 0..n {
       ...
    }
  }
}

``` 


```rust
/* 
Временная сложность алгоритма - экспоненциальная O(2^N)
 2^0 + 2^1 + 2^3 + ... + 2^N => O(2^N)

Очень плохая производительность. 
Вы хотите избежать подобных алгоритмов, но иногда у вас нет выбора. 
Добавление всего лишь одного бита к входным данным удваивает время работы. 
Пример: задача коммивояжера.

 cargo +nightly run
*/
fn main() {
    aasert_eq!(Some("11111110"),algo_2(254));

    let mut v = vec![5,7,1,4];
    algo_7_bubble_sort(&mut v);
    assert_eq!(vec![1,4,5,7],v);
}
```

```rust
/* 
Временная сложность алгоритма - факториал O(n!)
Они в основном используются в перестановочных и комбинаторных задачах.

Невыносимо медленно. Буквально на то, чтобы что-то сделать, уходит миллион лет.
В задаче "Комиивояжора" - построение всех возможных маршрутов, 
используется в картах или просто рекурсия
*/
fn factorial(n: i32) {
    for i in 0..n {
       factorial(n - 1)
    }
}

```

</details>






## Top 20 LeetCode Patterns



### 1. **Два указателя (Two Pointers)**

* Задачи на отсортированные массивы, подстроки, удаление дубликатов.
* Примеры:

  * Remove Duplicates from Sorted Array
  * 3Sum


### 2. **Скользящее окно (Sliding Window)**

* Поиск подотрезков массива/строки с нужными свойствами.
* Примеры:

  * Longest Substring Without Repeating Characters
  * Minimum Window Substring


### 3. **Бинарный поиск (Binary Search)**

* Не только на числах, но и на "ответе" (Binary Search on Answer).
* Примеры:

  * Search in Rotated Sorted Array
  * Median of Two Sorted Arrays


### 4. **Двоичный поиск по ответу (Binary Search on Answer)**

* Задачи типа "минимизируй/максимизируй условие".
* Примеры:

  * Koko Eating Bananas
  * Split Array Largest Sum


### 5. **Сортировка + Жадный алгоритм (Sorting + Greedy)**

* Когда порядок важен для оптимального решения.
* Примеры:

  * Meeting Rooms II
  * Non-overlapping Intervals


### 6. **Хеш-таблицы / Множества (Hashing)**

* Быстрая проверка принадлежности, подсчёты.
* Примеры:

  * Two Sum
  * Group Anagrams


### 7. **Стек (Stack)**

* Для пар скобок, монотонных стеков, Next Greater Element.
* Примеры:

  * Valid Parentheses
  * Largest Rectangle in Histogram


### 8. **Очередь / Дек (Queue / Deque)**

* Особенно для Sliding Window Maximum.
* Примеры:

  * Sliding Window Maximum
  * Rotting Oranges


### 9. **Динамическое программирование (DP)**

* Подразделы:

  * 1D DP (Fibonacci, House Robber)
  * 2D DP (Unique Paths, Coin Change)
  * DP на подстроках (Palindrome Partitioning)


### 10. **Разделяй и властвуй (Divide & Conquer)**

* Быстрые сортировки, работа с деревьями, подзадачи.
* Примеры:

  * Merge Sort
  * Maximum Subarray (Kadane / Divide & Conquer версия)


### 11. **DFS (поиск в глубину)**

* Графы, матрицы, деревья.
* Примеры:

  * Number of Islands
  * Path Sum

---

### 12. **BFS (поиск в ширину)**

* Кратчайшие пути, уровневый обход.
* Примеры:

  * Word Ladder
  * Binary Tree Level Order Traversal


### 13. **Backtracking**

* Перестановки, комбинации, N-Queens.
* Примеры:

  * Permutations
  * Combination Sum


### 14. **Union-Find (Disjoint Set Union)**

* Классификация графов, острова, Kruskal.
* Примеры:

  * Number of Connected Components in Graph
  * Redundant Connection


### 15. **Topological Sort (Kahn / DFS)**

* Задачи на зависимости.
* Примеры:

  * Course Schedule
  * Alien Dictionary


### 16. **Деревья поиска (BST)**

* Вставки, удаления, поиск K-th smallest/largest.
* Примеры:

  * Validate BST
  * Lowest Common Ancestor of a BST


### 17. **Heap / Приоритетная очередь**

* k-элементов, медианы, интервалы.
* Примеры:

  * Merge k Sorted Lists
  * Find Median from Data Stream


### 18. **Prefix Sum / Difference Array**

* Для подотрезков чисел или строк.
* Примеры:

  * Subarray Sum Equals K
  * Range Sum Query


### 19. **Bit Manipulation**

* Побитовые операции, подмножества.
* Примеры:

  * Single Number
  * Subsets II


### 20. **Математика и геометрия**

* Часто хитрые задачи решаются через формулы.
* Примеры:

  * Pow(x, n)
  * Rotate Image

p.s. учебный роадмап с подборкой 1–2 классических задач по каждому паттерну


### Links

[8 лучших алгоритмов, которые должен знать каждый программист](https://vc.ru/u/1389654-machine-learning/593476-8-luchshih-algoritmov-kotorye-dolzhen-znat-kazhdyy-programmist)

[AllAlgorithms](https://github.com/AllAlgorithms/rust) 

[geeksforgeeks.org](https://www.geeksforgeeks.org/fundamentals-of-algorithms/?ref=shm)

[Algorithmic complexity / Big-O / Asymptotic analysis](https://github.com/jwasham/coding-interview-university#algorithmic-complexity--big-o--asymptotic-analysis)

[Комбинаторика](https://brestprog.by/topics/combinatorics/)

[TheAlgorithms Rust](https://github.com/TheAlgorithms/Rust/tree/master)

[Visual algo](https://visualgo.net/en)

[Visual Algorithms](https://www.cs.usfca.edu/~galles/visualization/Algorithms.html)

[Vamonos visual Algorithms](https://rosulek.github.io/vamonos/)

[Algorithms e-maxx.ru](http://e-maxx.ru/algo/)

[Sorting-Algorithms on Rust](https://github.com/diptangsu/Sorting-Algorithms/tree/master/Rust)

[Sorting algorithm Wiki](https://en.wikipedia.org/wiki/Sorting_algorithm)

[Грокаем алгоритмы GitHub](https://github.com/egonSchiele/grokking_algorithms)

[Radix sort](https://www.youtube.com/watch?v=_KhZ7F-jOlI)

[swift-algorithm-club](https://github.com/kodecocodes/swift-algorithm-club/blob/master/Bubble%20Sort/README.markdown)

[Coursera Algorithms part 1](https://www.coursera.org/learn/algorithms-part1)

[Coursera Algorithms part 2](https://www.coursera.org/learn/algorithms-part2)

[neetcode roadmap](https://neetcode.io/roadmap)
